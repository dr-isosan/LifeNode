# LifeNode - Reinforcement Learning TasarÄ±mÄ±

## ğŸ“‹ Genel BakÄ±ÅŸ

**KiÅŸi B (AI & Environment Architect)** tarafÄ±ndan tasarlanan Reinforcement Learning (RL) altyapÄ±sÄ±, mesh network'te akÄ±llÄ± paket yÃ¶nlendirme (intelligent routing) iÃ§in DQN (Deep Q-Network) algoritmasÄ±nÄ± kullanÄ±r.

**AmaÃ§**: Her dÃ¼ÄŸÃ¼mÃ¼n, gelen bir paketi hangi komÅŸusuna yÃ¶nlendireceÄŸine dair optimal kararlar almasÄ±nÄ± Ã¶ÄŸrenmek.

---

## ğŸ—ï¸ AI ModÃ¼l YapÄ±sÄ±

```
src/ai/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ env.py              # RL Environment (Gymnasium tabanlÄ±)
â”œâ”€â”€ env_wrapper.py      # Environment sarmalayÄ±cÄ±larÄ±
â”œâ”€â”€ agent.py            # DQN Agent implementasyonu
â”œâ”€â”€ model.py            # Neural Network modeli
â”œâ”€â”€ state_encoder.py    # State representation encoder
â””â”€â”€ reward.py           # Reward fonksiyon sistemi
```

---

## ğŸ”„ RL Environment (Gymnasium)

### `LifeNodeEnv` SÄ±nÄ±fÄ±

**Dosya**: `src/ai/env.py`

Gymnasium API'sine uygun olarak tasarlanmÄ±ÅŸ environment. SimÃ¼lasyon ile AI arasÄ±ndaki kÃ¶prÃ¼ gÃ¶revini gÃ¶rÃ¼r.

#### **Temel Ã–zellikler**

```python
# Action Space: KomÅŸu seÃ§imi (Discrete)
action_space = Discrete(max_neighbors)  # 0-4 arasÄ± (5 komÅŸu iÃ§in)

# Observation Space: Normalize edilmiÅŸ state vektÃ¶rÃ¼
observation_space = Box(low=0, high=1, shape=(state_dim,), dtype=float32)
```

#### **Ana Metodlar**

##### `reset(seed, options) â†’ state, info`
Yeni bir episode baÅŸlatÄ±r. Paketi kaynak dÃ¼ÄŸÃ¼mde baÅŸlatÄ±r ve initial state dÃ¶ndÃ¼rÃ¼r.

**DÃ¶nen deÄŸerler:**
- `state`: Encode edilmiÅŸ durum vektÃ¶rÃ¼ (16 boyutlu)
- `info`: Ek bilgiler (debug iÃ§in)

**Ã–rnek kullanÄ±m:**
```python
env = LifeNodeEnv()
state, info = env.reset()
# state: [0.5, 0.8, 0.9, 0.2, ...] (16 float deÄŸer)
```

---

##### `step(action) â†’ next_state, reward, done, truncated, info`
Agent'Ä±n seÃ§tiÄŸi aksiyonu (komÅŸu seÃ§imi) uygular ve sonuÃ§larÄ± dÃ¶ndÃ¼rÃ¼r.

**Parametreler:**
- `action` (int): SeÃ§ilen komÅŸunun indeksi (0-4)

**DÃ¶nen deÄŸerler:**
- `next_state`: Yeni durum vektÃ¶rÃ¼
- `reward`: Ã–dÃ¼l skoru (float)
- `done`: Episode bitti mi? (paket ulaÅŸtÄ±/kayboldu)
- `truncated`: Zaman aÅŸÄ±mÄ± var mÄ±?
- `info`: Ek bilgiler

**AkÄ±ÅŸ:**
```
1. Paketi seÃ§ilen komÅŸuya yÃ¶nlendir
2. SimÃ¼lasyonu 1 adÄ±m ilerlet
3. Reward hesapla
4. Yeni state oluÅŸtur
5. Terminal durumu kontrol et (done?)
```

**Ã–rnek kullanÄ±m:**
```python
action = agent.act(state)  # Agent karar verir: 2 (komÅŸu #2)
next_state, reward, done, truncated, info = env.step(action)

if done:
    print("Paket hedefe ulaÅŸtÄ±!" if reward > 0 else "Paket kayboldu!")
```

---

## ğŸ§  State Representation (Durum GÃ¶sterimi)

### `StateEncoder` SÄ±nÄ±fÄ±

**Dosya**: `src/ai/state_encoder.py`

SimÃ¼lasyon dÃ¼nyasÄ±ndaki raw verileri (dÃ¼ÄŸÃ¼m pozisyonlarÄ±, komÅŸu listesi, vb.) **normalize edilmiÅŸ sayÄ±sal vektÃ¶re** dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

#### **State VektÃ¶r YapÄ±sÄ±**

```
State = [Hedefe_UzaklÄ±k, KomÅŸu1_Sinyal, KomÅŸu1_Enerji, KomÅŸu1_Kuyruk, ...]
        [    1 boyut   ,         3 boyut Ã— 5 komÅŸu = 15 boyut          ]

Toplam: 16 boyutlu vektÃ¶r
```

#### **Ã–zellik DetaylarÄ±**

| Ã–zellik                   | AÃ§Ä±klama                                 | Normalizasyon          | DeÄŸer AralÄ±ÄŸÄ± |
| ------------------------- | ---------------------------------------- | ---------------------- | ------------- |
| **Hedefe UzaklÄ±k**        | Mevcut dÃ¼ÄŸÃ¼mÃ¼n hedefe Euclidean mesafesi | `distance / 1000.0`    | [0.0, 1.0]    |
| **KomÅŸu Sinyal Kalitesi** | RF sinyal gÃ¼cÃ¼                           | RSSI â†’ 0-1 arasÄ±       | [0.0, 1.0]    |
| **KomÅŸu Enerji Seviyesi** | Batarya durumu                           | `battery / 100`        | [0.0, 1.0]    |
| **KomÅŸu Kuyruk Doluluk**  | Paket buffer doluluk oranÄ±               | `queue_len / capacity` | [0.0, 1.0]    |

#### **Padding MekanizmasÄ±**

EÄŸer bir dÃ¼ÄŸÃ¼mÃ¼n 5'ten az komÅŸusu varsa, eksik slotlar **dummy deÄŸerlerle** doldurulur:

```python
# Olmayan komÅŸu iÃ§in:
dummy_values = [0.0, 0.0, 1.0]  # Sinyal=0, Enerji=0, Kuyruk=Full
```

Bu sayede her state vektÃ¶rÃ¼ **sabit boyutlu** olur (Neural Network iÃ§in gerekli).

#### **Ã–rnek State**

```python
state = [
    0.45,      # Hedefe mesafe (normalize)
    0.85, 0.92, 0.1,   # KomÅŸu 1: GÃ¼Ã§lÃ¼ sinyal, yÃ¼ksek enerji, boÅŸ kuyruk
    0.62, 0.45, 0.6,   # KomÅŸu 2: Orta sinyal, orta enerji, yarÄ± dolu kuyruk
    0.0, 0.0, 1.0,     # KomÅŸu 3: YOK (padding)
    0.0, 0.0, 1.0,     # KomÅŸu 4: YOK (padding)
    0.0, 0.0, 1.0      # KomÅŸu 5: YOK (padding)
]
```

---

## ğŸ Reward System (Ã–dÃ¼l Fonksiyonu)

### `RewardSystem` SÄ±nÄ±fÄ±

**Dosya**: `src/ai/reward.py`

Agent'Ä±n davranÄ±ÅŸlarÄ±nÄ± ÅŸekillendiren Ã¶dÃ¼l mekanizmasÄ±. Ä°yi kararlar Ã¶dÃ¼llendirilir, kÃ¶tÃ¼ kararlar cezalandÄ±rÄ±lÄ±r.

#### **Matematiksel Model**

```
R(s, a, s') = {
    +100                           if paket_hedefe_ulaÅŸtÄ±
    -1                            if paket_kayboldu
    -5 - (1 - enerji) Ã— 50        otherwise (yolculuk devam ediyor)
}
```

#### **Ã–dÃ¼l BileÅŸenleri**

| BileÅŸen            | DeÄŸer  | AÃ§Ä±klama                        |
| ------------------ | ------ | ------------------------------- |
| **w_success**      | +100.0 | Paket baÅŸarÄ±yla teslim edildi   |
| **w_failure**      | -1.0   | Paket kayboldu/timeout oldu     |
| **w_step_penalty** | -5.0   | Her hop iÃ§in zaman cezasÄ±       |
| **w_energy**       | -50.0  | Enerji tÃ¼ketim cezasÄ± katsayÄ±sÄ± |

#### **Hesaplama MantÄ±ÄŸÄ±**

##### 1. BaÅŸarÄ± Durumu (Terminal State)
```python
if success:
    return +100.0  # Maksimum Ã¶dÃ¼l
```

##### 2. BaÅŸarÄ±sÄ±zlÄ±k Durumu (Terminal State)
```python
if failed:
    return -1.0  # KÃ¼Ã§Ã¼k ceza (Denemek iyidir)
```

##### 3. Devam Eden Durum (Intermediate State)
```python
# Zaman cezasÄ± (HÄ±zlÄ± olmayÄ± teÅŸvik eder)
step_reward = -5.0

# Enerji cezasÄ± (DÃ¼ÅŸÃ¼k enerjili dÃ¼ÄŸÃ¼mleri seÃ§meyi cezalandÄ±rÄ±r)
energy_penalty = (1.0 - energy_level) * (-50.0)

# Toplam Ã¶dÃ¼l
reward = step_reward + energy_penalty
```

**Ã–rnek:**
- KomÅŸu enerjisi **0.9** ise: `-5 - (0.1 Ã— 50) = -10.0`
- KomÅŸu enerjisi **0.2** ise: `-5 - (0.8 Ã— 50) = -45.0`

#### **TasarÄ±m Felsefesi**

1. **HÄ±zlÄ± teslim teÅŸvik edilir**: Her adÄ±m -5 ceza
2. **Enerji dengesi Ã¶nemli**: DÃ¼ÅŸÃ¼k bataryalÄ± dÃ¼ÄŸÃ¼mleri kullanmak maliyetli
3. **BaÅŸarÄ± yÃ¼ksek Ã¶dÃ¼llÃ¼**: Terminal baÅŸarÄ± +100, diÄŸer tÃ¼m cezalarÄ± telafi eder
4. **BaÅŸarÄ±sÄ±zlÄ±k cezasÄ± hafif**: Agent denemekten korkmamalÄ±

---

## ğŸ¤– DQN Agent

### `DQNAgent` SÄ±nÄ±fÄ±

**Dosya**: `src/ai/agent.py`

Deep Q-Network algoritmasÄ±nÄ± uygulayan ana agent sÄ±nÄ±fÄ±.

#### **Hiperparametreler**

```python
lr = 1e-3              # Learning rate (Ã–ÄŸrenme hÄ±zÄ±)
gamma = 0.99           # Discount factor (GeleceÄŸe Ã¶nem)
epsilon = 1.0          # Exploration rate (BaÅŸlangÄ±Ã§)
epsilon_min = 0.01     # Minimum exploration
epsilon_decay = 0.995  # Her episode'da epsilon Ã§arpanÄ±
buffer_size = 10000    # Replay memory kapasitesi
batch_size = 64        # Her eÄŸitim adÄ±mÄ±nda kullanÄ±lan sample sayÄ±sÄ±
```

#### **Ã‡ift AÄŸ Mimarisi**

DQN'de **2 ayrÄ± neural network** kullanÄ±lÄ±r:

1. **Policy Network** (`policy_net`): SÃ¼rekli eÄŸitilen, aksiyon seÃ§en aÄŸ
2. **Target Network** (`target_net`): Sabit kalan, hedef Q-deÄŸerleri hesaplayan aÄŸ

**Neden?** â†’ EÄŸitim stabilitesi. Target network periyodik olarak gÃ¼ncellenir.

#### **Ana Metodlar**

##### `act(state) â†’ action`
Epsilon-greedy strateji ile aksiyon seÃ§er.

```python
if random() < epsilon:
    return random_action()  # KeÅŸfet
else:
    return argmax(Q(state, a))  # En iyi bilinen aksiyonu seÃ§
```

##### `remember(state, action, reward, next_state, done)`
Deneyimi replay buffer'a kaydeder.

```python
memory.append((s, a, r, s', done))
```

##### `learn()`
Replay buffer'dan batch Ã§ekerek aÄŸÄ± eÄŸitir.

**Bellman Denklemi:**
```
Q(s, a) â† Q(s, a) + Î±[r + Î³Â·max_a' Q(s', a') - Q(s, a)]
```

**PyTorch implementasyonu:**
```python
current_q = policy_net(states).gather(1, actions)
next_q = target_net(next_states).max(1)[0]
target_q = rewards + gamma * next_q * (1 - dones)

loss = MSE(current_q, target_q)
loss.backward()
optimizer.step()
```

##### `update_epsilon()`
KeÅŸfetme oranÄ±nÄ± azaltÄ±r (exploitation'a geÃ§iÅŸ).

```python
epsilon = max(epsilon_min, epsilon * epsilon_decay)
```

---

## ğŸ§¬ Neural Network Modeli

### `DQN` SÄ±nÄ±fÄ± (Model)

**Dosya**: `src/ai/model.py`

3 katmanlÄ± fully-connected neural network.

#### **Mimari**

```
Input Layer (16)  â†’  Hidden Layer 1 (64)  â†’  Hidden Layer 2 (64)  â†’  Output Layer (5)
                       [ReLU]                     [ReLU]                [Linear]
```

**Parametreler:**
- **Input**: State vektÃ¶rÃ¼ (16 boyut)
- **Output**: Q-deÄŸerleri (5 aksiyon iÃ§in)

**Ã–rnek Ã§Ä±ktÄ±:**
```python
Q_values = model(state)
# [0.23, 0.87, 0.45, 0.12, 0.34]
#   â†“     â†‘     â†“     â†“     â†“
# KomÅŸu 1'in en yÃ¼ksek Q-deÄŸeri var â†’ SeÃ§!
```

#### **Aktivasyon FonksiyonlarÄ±**

- **ReLU** (Rectified Linear Unit): `f(x) = max(0, x)`
  - Avantaj: HÄ±zlÄ± Ã¶ÄŸrenme, gradient vanishing problemi yok
  - Gizli katmanlarda kullanÄ±lÄ±r

- **Linear** (Son katman): Ham Q-deÄŸerleri gerekli
  - Aktivasyon yok, Ã§Ã¼nkÃ¼ Q-deÄŸerleri negatif olabilir

---

## ğŸ”„ EÄŸitim AkÄ±ÅŸÄ± (Training Loop)

```python
for episode in range(num_episodes):
    state, _ = env.reset()
    total_reward = 0

    while not done:
        # 1. Aksiyon seÃ§ (Epsilon-greedy)
        action = agent.act(state)

        # 2. Ortamda uygula
        next_state, reward, done, truncated, info = env.step(action)

        # 3. Deneyimi kaydet
        agent.remember(state, action, reward, next_state, done)

        # 4. Ã–ÄŸren (Replay buffer yeterliyse)
        agent.learn()

        # 5. State gÃ¼ncelle
        state = next_state
        total_reward += reward

    # Episode bitti
    agent.update_epsilon()
    print(f"Episode {episode}: Reward = {total_reward}, Epsilon = {agent.epsilon}")
```

---

## ğŸ“Š Beklenen Performans Metrikleri

### EÄŸitim SÄ±rasÄ±nda Ä°zlenecek Metrikler

1. **Episode Reward**: BÃ¶lÃ¼m baÅŸÄ±na toplam Ã¶dÃ¼l
   - BaÅŸlangÄ±Ã§: ~-100 (rastgele routing)
   - Hedef: +50-80 (akÄ±llÄ± routing)

2. **Success Rate**: Paketlerin teslim oranÄ±
   - BaÅŸlangÄ±Ã§: %10-20
   - Hedef: %80-90

3. **Average Hop Count**: Ortalama sÄ±Ã§rama sayÄ±sÄ±
   - BaÅŸlangÄ±Ã§: 15-20 hop
   - Hedef: 3-5 hop (optimal path)

4. **Epsilon Decay**: KeÅŸfetme oranÄ±
   - BaÅŸlangÄ±Ã§: 1.0 (tamamen rastgele)
   - Final: 0.01 (tamamen Ã¶ÄŸrenilmiÅŸ)

---

## ğŸ”§ Entegrasyon PlanÄ± (KiÅŸi A ile)

### Hafta 2+ Hedefleri

1. **SimÃ¼lasyon-AI KÃ¶prÃ¼sÃ¼**
   - `Network` sÄ±nÄ±fÄ±ndan gerÃ§ek dÃ¼ÄŸÃ¼m verilerini Ã§ekme
   - Mock veriler yerine real-time state encoding

2. **Custom Callback Sistemi**
   - Her paket yÃ¶nlendirmede agent'Ä± Ã§aÄŸÄ±rma
   - `dummy_routing()` yerine `agent.act()` kullanÄ±mÄ±

3. **EÄŸitim Pipeline'Ä±**
   - Otomatik test scenario Ã¼retimi
   - Batch training (1000+ episode)
   - Model checkpoint kaydetme

4. **KarÅŸÄ±laÅŸtÄ±rma Testleri**
   - Baseline: Rastgele routing
   - Baseline: Greedy (en yakÄ±n komÅŸu)
   - DQN: Ã–ÄŸrenilmiÅŸ strateji

---

## ğŸ“š Matematiksel Notasyon

### State Space
```
S âˆˆ â„^16  (16-boyutlu sÃ¼rekli state uzayÄ±)
```

### Action Space
```
A = {0, 1, 2, 3, 4}  (5 discrete aksiyon)
```

### Bellman Optimality Equation
```
Q*(s, a) = E[r + Î³Â·max_a' Q*(s', a') | s, a]
```

### Loss Function (MSE)
```
L(Î¸) = E[(r + Î³Â·max_a' Q(s', a'; Î¸â») - Q(s, a; Î¸))Â²]
```

Burada:
- `Î¸`: Policy network parametreleri
- `Î¸â»`: Target network parametreleri (frozen)

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Environment Testi

```python
from src.ai.env import LifeNodeEnv

env = LifeNodeEnv()
state, _ = env.reset()

print(f"State boyutu: {len(state)}")  # 16
print(f"Action sayÄ±sÄ±: {env.action_space.n}")  # 5

for _ in range(10):
    action = env.action_space.sample()  # Rastgele aksiyon
    next_state, reward, done, truncated, info = env.step(action)
    print(f"Action: {action}, Reward: {reward:.2f}, Done: {done}")
    if done:
        break
```

### Agent EÄŸitimi (BasitleÅŸtirilmiÅŸ)

```python
from src.ai.env import LifeNodeEnv
from src.ai.agent import DQNAgent

env = LifeNodeEnv()
agent = DQNAgent(state_dim=16, action_dim=5)

for episode in range(100):
    state, _ = env.reset()
    total_reward = 0

    while True:
        action = agent.act(state)
        next_state, reward, done, _, _ = env.step(action)

        agent.remember(state, action, reward, next_state, done)
        agent.learn()

        state = next_state
        total_reward += reward

        if done:
            break

    agent.update_epsilon()
    print(f"Episode {episode}: Reward = {total_reward:.2f}")
```

---

## ğŸ¯ SonuÃ§

Bu RL tasarÄ±mÄ±, LifeNode projesinin "akÄ±llÄ± yÃ¶nlendirme" kÄ±smÄ±nÄ± oluÅŸturur. DQN algoritmasÄ± sayesinde her dÃ¼ÄŸÃ¼m, geÃ§miÅŸ deneyimlerden Ã¶ÄŸrenerek optimal routing kararlarÄ± alabilecektir.

**KiÅŸi A** ile entegre edildiÄŸinde:
- GerÃ§ek topoloji verileri kullanÄ±lacak
- Dinamik node failure senaryolarÄ±nda test edilecek
- Performans metrikleri karÅŸÄ±laÅŸtÄ±rÄ±lacak

**Hafta 1 Tamamlanan GÃ¶revler:**
- âœ… Environment iskeleti (Gymnasium uyumlu)
- âœ… State representation (StateEncoder)
- âœ… Reward fonksiyonu (matematiksel model)
- âœ… DQN agent ve model implementasyonu
- âœ… RL tasarÄ±m dokÃ¼mantasyonu

---

**HazÄ±rlayan**: KiÅŸi B (AI & Environment Architect)
**Tarih**: Hafta 1
**Versiyon**: 1.0
